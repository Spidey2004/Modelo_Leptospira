{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spidey2004/Modelo_Leptospira/blob/Spidey2004-patch-1.0/class_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca4f2117"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "This section includes the necessary imports, mounting of Git Hub, and definition of file paths and directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "751780d6",
        "outputId": "d3b743c7-9bbf-4f98-8cdd-f63a50d78836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ncbi-blast+ is already the newest version (2.12.0+ds-3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "fatal: destination path 'Modelo_Leptospira' already exists and is not an empty directory.\n",
            "Cloned repository path: ./Modelo_Leptospira\n",
            "Model directory path: ./Modelo_Leptospira/models\n",
            "Protein sequences directory path: ./Modelo_Leptospira/multi_fastas\n",
            "New genome file path: ./Modelo_Leptospira/data/120_Brem_307.fas\n",
            "Training file path: ./Modelo_Leptospira/data/serogroup_averages - Sheet2 (1).csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import subprocess\n",
        "import shutil\n",
        "import tempfile\n",
        "import traceback\n",
        "# Install NCBI BLAST+ if not already installed\n",
        "# This command is necessary regardless of how files are obtained, for the BLAST functionality.\n",
        "!apt-get update\n",
        "!apt-get install ncbi-blast+ -y\n",
        "# Clone the github repo\n",
        "!git clone --branch Spidey2004-patch-1.0 https://github.com/Spidey2004/Modelo_Leptospira.git\n",
        "\n",
        "# Define local paths based on the cloned GitHub repository\n",
        "# The repository will be cloned into a directory named 'Modelo_Leptospira'\n",
        "cloned_repo_path = \"./Modelo_Leptospira\"\n",
        "\n",
        "# Define paths for the directories and files within the cloned repository\n",
        "model_directory_path = os.path.join(cloned_repo_path, \"models\")\n",
        "protein_sequences_directory_path = os.path.join(cloned_repo_path, \"multi_fastas\")\n",
        "# The specific genome file path will be within the data directory of the cloned repo\n",
        "new_genome_file_path = os.path.join(cloned_repo_path, \"data\", \"120_Brem_307.fas\")\n",
        "# The training data file path will be within the data directory of the cloned repo\n",
        "training_file_path = os.path.join(cloned_repo_path, \"data\", \"serogroup_averages - Sheet2 (1).csv\") # Corrected filename\n",
        "\n",
        "print(f\"Cloned repository path: {cloned_repo_path}\")\n",
        "print(f\"Model directory path: {model_directory_path}\")\n",
        "print(f\"Protein sequences directory path: {protein_sequences_directory_path}\")\n",
        "print(f\"New genome file path: {new_genome_file_path}\")\n",
        "print(f\"Training file path: {training_file_path}\")\n",
        "# Display all rows\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "984f061d"
      },
      "source": [
        "## Data Loading and Preprocessing Functions\n",
        "\n",
        "These functions handle the loading, verification, and preprocessing of the data for both training (used here to get training columns) and prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7Zwn6l6gb_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import traceback # Import traceback for detailed error reporting\n",
        "\n",
        "def preprocess_for_prediction(table):\n",
        "    \"\"\"\n",
        "    Preprocess data for machine learning prediction.\n",
        "\n",
        "    Args:\n",
        "        table (pd.DataFrame): The input DataFrame for prediction.\n",
        "\n",
        "    Returns:\n",
        "\n",
        "               original_indices (list): List of original row indices from the input DataFrame.\n",
        "    \"\"\"\n",
        "    print(\"Starting preprocessing for prediction...\")\n",
        "    # Drop the specified columns that are not features, ignoring errors if they don't exist\n",
        "    columns_to_exclude = ['Strain', 'Serogroup', 'Species']\n",
        "    processed_table = table.drop(columns=columns_to_exclude, errors='ignore').copy()\n",
        "    print(f\"  - Columns '{columns_to_exclude}' removed. Current shape: {processed_table.shape}\")\n",
        "\n",
        "    # Ensure all columns are numeric, coercing non-numeric values to NaN\n",
        "    print(\"  - Converting non-numeric columns to numeric...\")\n",
        "    non_numeric_columns = processed_table.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "    if non_numeric_columns:\n",
        "        print(f\"    - Non-numeric columns detected: {non_numeric_columns}\")\n",
        "        for col in non_numeric_columns:\n",
        "             processed_table[col] = pd.to_numeric(processed_table[col], errors='coerce')\n",
        "    print(\"  - Column conversion completed.\")\n",
        "\n",
        "    # Drop rows with any remaining missing values (NaNs)\n",
        "    rows_before_dropna = processed_table.shape[0]\n",
        "    processed_table = processed_table.dropna()\n",
        "    rows_after_dropna = processed_table.shape[0]\n",
        "    if rows_after_dropna < rows_before_dropna:\n",
        "        print(f\"  - Removed {rows_before_dropna - rows_after_dropna} rows with missing values.\")\n",
        "    print(f\"  - Shape after removing missing values: {processed_table.shape}\")\n",
        "\n",
        "    # Drop the 'class' column if it exists, as it's not a feature for prediction\n",
        "    if 'class' in processed_table.columns:\n",
        "        print(\"  - 'class' column found. Removing...\")\n",
        "        processed_table = processed_table.drop(columns=['class'])\n",
        "        print(f\"  - 'class' column removed. Current shape: {processed_table.shape}\")\n",
        "    else:\n",
        "        print(\"  - 'class' column not found. Proceeding.\")\n",
        "\n",
        "    # Debugging: Print columns before prediction to verify\n",
        "    print(\"\\n  - Columns in DataFrame before prediction:\")\n",
        "    print(processed_table.columns.tolist())\n",
        "    print(f\"  - Número de colunas antes de predição: {processed_table.shape[1]}\")\n",
        "\n",
        "    # Ensure the order of columns matches the training data features.\n",
        "    # This is crucial for consistent prediction.\n",
        "    # A robust approach requires saving and loading the list of training feature column names.\n",
        "    # Assuming for now that the remaining columns are in the correct relative order.\n",
        "\n",
        "    print(\"Preprocessing for prediction completed.\")\n",
        "    # Return the feature matrix and the list of original indices for the rows that were kept\n",
        "    return processed_table, processed_table.index.tolist()\n",
        "\n",
        "def parse_fasta_to_tuples(fasta_path):\n",
        "    \"\"\"\n",
        "    Reads a .fasta file and returns a list of (header, sequence) tuples.\n",
        "\n",
        "    Args:\n",
        "        fasta_path (str): Path to the .fasta file.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples, where each tuple contains the sequence header\n",
        "              (without the '>' symbol) and the sequence string.\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    header = None\n",
        "    seq = \"\"\n",
        "\n",
        "    # Open and read the fasta file line by line\n",
        "    with open(fasta_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.startswith(\">\"):\n",
        "                # If a header and sequence were previously collected, store them\n",
        "                if header and seq:\n",
        "                    sequences.append((header, seq))\n",
        "                # Start a new sequence entry with the current header\n",
        "                header = line[1:]  # remove the \">\" character\n",
        "                seq = \"\"\n",
        "            else:\n",
        "                # Append non-header lines to the current sequence\n",
        "                seq += line\n",
        "        # Add the last sequence in the file after the loop finishes\n",
        "        if header and seq:\n",
        "            sequences.append((header, seq))\n",
        "\n",
        "    return sequences\n",
        "\n",
        "def build_sequence_dictionary(protein_sequences_directory_path):\n",
        "    \"\"\"\n",
        "    Reads all .fasta files in a directory and builds a dictionary mapping\n",
        "    serogroup names (derived from filenames) to lists of protein sequences.\n",
        "\n",
        "    Args:\n",
        "        protein_sequences_directory_path (str): Path to the directory containing\n",
        "                                                the protein sequence .fasta files.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are serogroup names and values are lists\n",
        "              of (header, sequence) tuples parsed from the .fasta files.\n",
        "    \"\"\"\n",
        "    protein_sequences_by_serogroup = {}\n",
        "\n",
        "    # Iterate through each file in the specified directory\n",
        "    for filename in os.listdir(protein_sequences_directory_path):\n",
        "        # Process files with common FASTA extensions\n",
        "        if filename.endswith(\".fasta\") or filename.endswith(\".fa\"):\n",
        "            fasta_path = os.path.join(protein_sequences_directory_path, filename)\n",
        "\n",
        "            # Define serogroup name from the filename (assuming name before the first \"_\" or extension)\n",
        "            # Example: 'Ballum_proteins.fasta' -> 'Ballum'\n",
        "            # Or 'Ballum.fasta' -> 'Ballum'\n",
        "            # Use filename without extension as the key\n",
        "            serogroup_name = os.path.splitext(filename)[0]\n",
        "\n",
        "\n",
        "            # Parse the fasta file into a list of (header, sequence) tuples\n",
        "            sequence_list = parse_fasta_to_tuples(fasta_path)\n",
        "\n",
        "            # Add the parsed sequences to the dictionary under the serogroup name key\n",
        "            protein_sequences_by_serogroup[serogroup_name] = sequence_list\n",
        "\n",
        "            print(f\"Processed {filename} ({len(sequence_list)} sequences)\")\n",
        "\n",
        "    return protein_sequences_by_serogroup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b64c986"
      },
      "source": [
        "## BLAST and Prediction Pipeline Functions\n",
        "\n",
        "These functions handle running BLAST, parsing its output, and making predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9ZYWeTehLcq"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import tempfile\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "def run_tblastn(genome_archive_path, protein_sequences_by_serogroup, output_file_path):\n",
        "    \"\"\"\n",
        "    Runs tblastn with the given genome archive against the provided protein sequences,\n",
        "    grouped by serogroup, and saves the output to a specified file.\n",
        "\n",
        "    Args:\n",
        "        genome_archive_path (str): Path to the genome archive file (e.g., .fasta, .fna).\n",
        "        protein_sequences_by_serogroup (dict): A dictionary where keys are serogroup names\n",
        "                                              and values are lists of (header, sequence) tuples.\n",
        "        output_file_path (str): Path to save the tblastn output.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if tblastn ran successfully for all serogroups, False otherwise.\n",
        "    \"\"\"\n",
        "    print(f\"Starting tblastn for genome archive: {os.path.basename(genome_archive_path)}\")\n",
        "    print(f\"  - Saving tblastn output to: {output_file_path}\")\n",
        "\n",
        "    # Create a temporary directory for BLAST database files\n",
        "    blast_db_dir = tempfile.mkdtemp()\n",
        "    # print(f\"  - Created temporary BLAST database directory: {blast_db_dir}\") # Removed debug print\n",
        "\n",
        "    # Define paths for temporary files within the temporary directory\n",
        "    genome_fasta = os.path.join(blast_db_dir, \"genome.fasta\")\n",
        "    database_name = os.path.join(blast_db_dir, \"genome_db\")\n",
        "\n",
        "    # Extract genome from archive (assuming it's a fasta file inside) or copy if already fasta\n",
        "    try:\n",
        "        if genome_archive_path.endswith('.gz'):\n",
        "             # Use gunzip to decompress and save to the temporary fasta file\n",
        "             subprocess.run(['gunzip', '-c', genome_archive_path], stdout=open(genome_fasta, 'w'), check=True)\n",
        "             # print(f\"  - Extracted genome from {os.path.basename(genome_archive_path)} to {genome_fasta}\") # Removed debug print\n",
        "        else:\n",
        "            # Just copy the file if it's not compressed\n",
        "            subprocess.run(['cp', genome_archive_path, genome_fasta], check=True)\n",
        "            # print(f\"  - Copied genome file {os.path.basename(genome_archive_path)} to {genome_fasta}\") # Removed debug print\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"  - Error extracting or copying genome from archive: {e}\")\n",
        "        shutil.rmtree(blast_db_dir) # Clean up the temporary directory\n",
        "        return False\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  - Error: Could not find command to extract or copy genome file.\")\n",
        "        shutil.rmtree(blast_db_dir) # Clean up the temporary directory\n",
        "        return False\n",
        "\n",
        "\n",
        "    # Create BLAST database from the genome fasta file\n",
        "    try:\n",
        "        print(\"  - Creating BLAST database...\")\n",
        "        # Use subprocess.Popen with pipes to capture potential large stdout/stderr\n",
        "        makeblastdb_process = subprocess.Popen([\n",
        "            \"makeblastdb\",\n",
        "            \"-in\", genome_fasta,\n",
        "            \"-dbtype\", \"nucl\", # Nucleotide database type\n",
        "            \"-out\", database_name # Output database name\n",
        "        ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        stdout, stderr = makeblastdb_process.communicate()\n",
        "        if makeblastdb_process.returncode != 0:\n",
        "             print(f\"  - Error creating BLAST database: {stderr.decode()}\")\n",
        "             shutil.rmtree(blast_db_dir) # Clean up the temporary directory\n",
        "             return False\n",
        "        print(\"  - BLAST database created successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  - Error: 'makeblastdb' command not found. Ensure NCBI BLAST+ is installed and in your PATH.\")\n",
        "        shutil.rmtree(blast_db_dir) # Clean up the temporary directory\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"  - An unexpected error occurred during BLAST database creation: {e}\")\n",
        "        shutil.rmtree(blast_db_dir) # Clean up the temporary directory\n",
        "        return False\n",
        "\n",
        "\n",
        "    # Run tblastn for each serogroup's protein sequences against the genome database\n",
        "    success = True\n",
        "    # Ensure the output file is empty before appending results from each serogroup\n",
        "    try:\n",
        "        with open(output_file_path, 'w') as f:\n",
        "            pass # Create or clear the file by opening in write mode and closing\n",
        "    except IOError as e:\n",
        "        print(f\"  - Error: Could not write to output file {output_file_path}: {e}\")\n",
        "        shutil.rmtree(blast_db_dir) # Clean up the temporary directory\n",
        "        return False\n",
        "\n",
        "\n",
        "    for serogroup_name, sequences in protein_sequences_by_serogroup.items():\n",
        "        if not sequences:\n",
        "            # print(f\"  - Skipping serogroup {serogroup_name}: No sequences found.\") # Removed debug print\n",
        "            continue\n",
        "\n",
        "        # Define temporary FASTA file path for the current serogroup's proteins within the DB directory\n",
        "        serogroup_protein_fasta = os.path.join(blast_db_dir, f\"{serogroup_name}_proteins.fasta\")\n",
        "\n",
        "        # Create temporary FASTA file for the current serogroup's protein sequences\n",
        "        with open(serogroup_protein_fasta, \"w\") as f:\n",
        "            # Iterate through the list of (header, sequence) tuples\n",
        "            for item in sequences:\n",
        "                # Ensure the item is in the expected tuple format\n",
        "                if isinstance(item, tuple) and len(item) == 2:\n",
        "                    header, sequence = item\n",
        "                    # Append the serogroup name to the header for identification in BLAST output\n",
        "                    # Take only the first part of the header if it contains spaces\n",
        "                    cleaned_header = header.split()[0]\n",
        "                    f.write(f\">{cleaned_header}_{serogroup_name}\\n{sequence}\\n\")\n",
        "                else:\n",
        "                    # print(f\"  - Warning: Skipping unexpected item format in sequences for {serogroup_name}: {item}\") # Removed debug print\n",
        "                    success = False # Indicate an issue but try to continue\n",
        "\n",
        "        # Run tblastn for the current serogroup's proteins against the genome database\n",
        "        try:\n",
        "            # Use 'a' mode to append tblastn results to the main output file\n",
        "            with open(output_file_path, 'a') as outfile:\n",
        "                 tblastn_process = subprocess.Popen([\n",
        "                     \"tblastn\",\n",
        "                     \"-query\", serogroup_protein_fasta, # Protein sequences for the current serogroup\n",
        "                     \"-db\", database_name, # Genome database\n",
        "                     \"-outfmt\", \"6 qseqid sseqid pident length qlen\", # Output format\n",
        "                     #\"-max_target_seqs\", \"1\", # Output format: query seq id, subject seq id, percent identity, alignment length, query length\n",
        "                 ], stdout=outfile, stderr=subprocess.PIPE) # Direct stdout to the output file and capture stderr\n",
        "                 stdout, stderr = tblastn_process.communicate()\n",
        "\n",
        "                 if tblastn_process.returncode != 0:\n",
        "                      print(f\"  - Error running tblastn for {serogroup_name}: {stderr.decode()}\")\n",
        "                      success = False # Mark as failed but continue with other serogroups\n",
        "\n",
        "            # print(f\"  - tblastn completed for serogroup {serogroup_name}.\") # Removed debug print\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"  - Error: 'tblastn' command not found. Ensure NCBI BLAST+ is installed and in your PATH.\")\n",
        "            success = False\n",
        "            break # Cannot continue if tblastn command is missing\n",
        "        except Exception as e:\n",
        "            print(f\"  - An unexpected error occurred during tblastn for {serogroup_name}: {e}\")\n",
        "            success = False # Mark as failed but continue with other serogroups\n",
        "\n",
        "        # Clean up the temporary serogroup protein FASTA file after use\n",
        "        os.remove(serogroup_protein_fasta)\n",
        "\n",
        "    # Clean up the temporary BLAST database directory and its contents\n",
        "    shutil.rmtree(blast_db_dir)\n",
        "    # print(f\"  - Cleaned up temporary BLAST database directory: {blast_db_dir}\") # Removed debug print\n",
        "\n",
        "    print(\"tblastn execution completed.\")\n",
        "    return success\n",
        "\n",
        "# NOTE: The cell that calls this function (cell_id: Kr-w5BdKjVUC) will need to be re-run after this modification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr-w5BdKjVUC"
      },
      "outputs": [],
      "source": [
        "def predict_serogroup_from_genome(genome_archive_path, protein_sequences_by_serogroup, loaded_model, training_feature_columns):\n",
        "    \"\"\"\n",
        "    Runs tblastn on a genome archive, processes the output into a feature vector\n",
        "    with columns matching the training data, and predicts the serogroup using\n",
        "    a loaded model.\n",
        "\n",
        "    Args:\n",
        "        genome_archive_path (str): Path to the genome archive file (e.g., .fasta, .fna).\n",
        "        protein_sequences_by_serogroup (dict): A dictionary where keys are serogroup names\n",
        "                                              and values are lists of (header, sequence) tuples.\n",
        "        loaded_model: The pre-trained machine learning model.\n",
        "        training_feature_columns (list): A list of column names used as features during training.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame or None: A DataFrame containing the prediction and probabilities,\n",
        "                              or None if the process fails.\n",
        "    \"\"\"\n",
        "    print(f\"Starting prediction pipeline for genome: {os.path.basename(genome_archive_path)}\")\n",
        "\n",
        "    # Define a temporary file path for the combined tblastn output\n",
        "    # We will still save to a file, but also print its content for debugging\n",
        "    blast_output_file = \"/tmp/blast_output.txt\"\n",
        "\n",
        "    # Run tblastn using the protein sequences grouped by serogroup against the genome\n",
        "    run_tblastn_result = run_tblastn(genome_archive_path, protein_sequences_by_serogroup, blast_output_file)\n",
        "\n",
        "    if not run_tblastn_result:\n",
        "        print(\"  - tblastn execution failed. Aborting prediction.\")\n",
        "        return None\n",
        "\n",
        "    # --- Adicionado para imprimir a saída do BLAST para depuração ---\n",
        "    # Removed debug print of BLAST output file content\n",
        "    # --------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Parse the combined BLAST output and create a feature vector DataFrame\n",
        "    # Pass the protein_sequences_by_serogroup dictionary to the parsing function for mapping\n",
        "    # Remove the loaded_scaler argument\n",
        "    feature_vector_df = parse_blast_output_and_create_feature_vector(blast_output_file, protein_sequences_by_serogroup, training_feature_columns)\n",
        "\n",
        "    if feature_vector_df is None:\n",
        "        print(\"  - Failed to create feature vector from BLAST output. Aborting prediction.\")\n",
        "        return None\n",
        "\n",
        "    # Display the feature vector DataFrame containing the mean BLAST results per serogroup\n",
        "    print(f\"\\n--- Mean BLAST Pident per Serogroup for {os.path.basename(genome_archive_path)} ---\")\n",
        "    display(feature_vector_df)\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    # Preprocess the feature vector (removed scaling)\n",
        "    print(\"  - Preparing feature vector for prediction...\")\n",
        "    try:\n",
        "        # Ensure the feature vector has the same columns and order as the training data before prediction\n",
        "        # The parsing function should handle this, but reindex is a safeguard.\n",
        "        if not feature_vector_df.columns.equals(pd.Index(training_feature_columns)):\n",
        "             print(\"  - Warning: Feature vector columns do not match training columns. Attempting reindex.\")\n",
        "             # Reindex to match training columns, filling missing ones with 0.0\n",
        "             feature_vector_df = feature_vector_df.reindex(columns=training_feature_columns, fill_value=0.0)\n",
        "             # print(\"  - Feature vector reindexed.\") # Removed debug print\n",
        "\n",
        "        # Use the feature vector directly without scaling\n",
        "        X_pred = feature_vector_df.values\n",
        "        print(\"  - Feature vector ready for prediction.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  - Error preparing feature vector: {e}\")\n",
        "        print(\"  - Ensure the feature vector columns and order match the model's expected input.\")\n",
        "        # Print traceback for debugging\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "    # Make predictions using the loaded machine learning model\n",
        "    # print(\"  - Making predictions...\") # Removed debug print\n",
        "    try:\n",
        "        predictions = loaded_model.predict(X_pred)\n",
        "        prediction_probabilities = loaded_model.predict_proba(X_pred)\n",
        "        # print(\"  - Predictions made successfully.\") # Removed debug print\n",
        "    except Exception as e:\n",
        "        print(f\"  - Error making predictions: {e}\")\n",
        "        print(\"  - Ensure the feature vector shape matches the model's expected input.\")\n",
        "        # Print traceback for debugging\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "    # Create a DataFrame to store the prediction results and probabilities\n",
        "    # Use the genome file name as identification for the prediction row\n",
        "    genome_name = os.path.basename(genome_archive_path)\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'Genome': [genome_name],\n",
        "        'Predicted_Class': predictions,\n",
        "    })\n",
        "\n",
        "    # Add columns for prediction probabilities for each class\n",
        "    # Ensure the class labels from the model are used for column names\n",
        "    if hasattr(loaded_model, 'classes_'):\n",
        "        for i, class_label in enumerate(loaded_model.classes_):\n",
        "             predictions_df[f'Probability_Class_{class_label}'] = prediction_probabilities[:, i]\n",
        "    else:\n",
        "        print(\"  - Warning: Model does not have 'classes_' attribute. Probability columns might not be correctly labeled.\")\n",
        "        # Fallback to generic column names if class labels are not available\n",
        "        for i in range(prediction_probabilities.shape[1]):\n",
        "             predictions_df[f'Probability_Class_{i+1}'] = prediction_probabilities[:, i]\n",
        "\n",
        "    print(\"Prediction pipeline completed.\")\n",
        "    # Display the resulting predictions DataFrame\n",
        "    # Removed display of final prediction DataFrame from this function,\n",
        "    # as the overall results are concatenated and displayed later.\n",
        "    # display(predictions_df)\n",
        "\n",
        "    return predictions_df\n",
        "\n",
        "# NOTE: The cell that calls this function (cell_id: 2-Xp9xLGi_b1) will need to be re-run after this modification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M4SIbMVjJ_r",
        "outputId": "7885225a-22a1-4d75-c832-6abcde5c5ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading protein sequences from directory: ./Modelo_Leptospira/multi_fastas\n",
            "  - Reading file: cl_1_Ranarum_GCF_000332415.1.fasta for serogroup: cl_1_Ranarum_GCF_000332415.1\n",
            "  - Reading file: cl_1_Pyrogenes_GCF_022559725.1.fasta for serogroup: cl_1_Pyrogenes_GCF_022559725.1\n",
            "  - Reading file: cl_3_Hebdomadis_GCF_000244115.1.fasta for serogroup: cl_3_Hebdomadis_GCF_000244115.1\n",
            "  - Reading file: cl_1_Ballum_GCF_009884235.1.fasta for serogroup: cl_1_Ballum_GCF_009884235.1\n",
            "  - Reading file: cl_4_Tarassovi_GCF_024704545.1.fasta for serogroup: cl_4_Tarassovi_GCF_024704545.1\n",
            "  - Reading file: cl_2_Pomona_GCF_001857845.1.fasta for serogroup: cl_2_Pomona_GCF_001857845.1\n",
            "  - Reading file: cl_2_Autumnalis_GCF_022819425.1.fasta for serogroup: cl_2_Autumnalis_GCF_022819425.1\n",
            "  - Reading file: cl_4_Bataviae_GCF_014858865.1.fasta for serogroup: cl_4_Bataviae_GCF_014858865.1\n",
            "  - Reading file: cl_1_Icterohaemorrhagiae_GCF_000231175.1.fasta for serogroup: cl_1_Icterohaemorrhagiae_GCF_000231175.1\n",
            "  - Reading file: cl_1_Sarmin_GCF_030023765.1.fasta for serogroup: cl_1_Sarmin_GCF_030023765.1\n",
            "  - Reading file: cl_2_Djasiman_GCF_000216195.1.fasta for serogroup: cl_2_Djasiman_GCF_000216195.1\n",
            "  - Reading file: cl_1_Canicola_GCF_008831465.1.fasta for serogroup: cl_1_Canicola_GCF_008831465.1\n",
            "  - Reading file: cl_1_Manhao_GCF_000243815.2.fasta for serogroup: cl_1_Manhao_GCF_000243815.2\n",
            "  - Reading file: cl_2_Cynopteri_GCF_000243695.2.fasta for serogroup: cl_2_Cynopteri_GCF_000243695.2\n",
            "  - Reading file: cl_2_Panama_GCF_000306255.2.fasta for serogroup: cl_2_Panama_GCF_000306255.2\n",
            "  - Reading file: cl_3_Mini_GCF_000306675.2.fasta for serogroup: cl_3_Mini_GCF_000306675.2\n",
            "  - Reading file: cl_2_Australis_GCF_022819715.1.fasta for serogroup: cl_2_Australis_GCF_022819715.1\n",
            "  - Reading file: cl_3_Sejroe_GCF_003254845.1.fasta for serogroup: cl_3_Sejroe_GCF_003254845.1\n",
            "  - Reading file: cl_4_Shermani_GCF_000313175.2.fasta for serogroup: cl_4_Shermani_GCF_000313175.2\n",
            "  - Reading file: cl_2_Grippotyphosa_GCF_024526015.1.fasta for serogroup: cl_2_Grippotyphosa_GCF_024526015.1\n",
            "  - Reading file: cl_1_Celledoni_GCF_022344045.1.fasta for serogroup: cl_1_Celledoni_GCF_022344045.1\n",
            "  - Reading file: cl_1_Javanica_GCF_024722275.1.fasta for serogroup: cl_1_Javanica_GCF_024722275.1\n",
            "Loaded protein sequences for 22 serogroups from directory.\n"
          ]
        }
      ],
      "source": [
        "# Define the path to the directory containing protein sequences for BLAST\n",
        "# This path is now set in the initial setup cell after cloning.\n",
        "# protein_sequences_directory_path = \"https://github.com/Spidey2004/Modelo_Leptospira/tree/Spidey2004-patch-1.0/multi_fastas\" # Old path\n",
        "\n",
        "# Initialize an empty dictionary to store sequences grouped by serogroup\n",
        "protein_sequences_by_serogroup = {}\n",
        "\n",
        "# Check if the specified path is a directory\n",
        "# We are now using the local path from the cloned repository.\n",
        "if not os.path.isdir(protein_sequences_directory_path):\n",
        "    print(f\"Error: Directory not found at {protein_sequences_directory_path}\")\n",
        "else:\n",
        "    print(f\"Reading protein sequences from directory: {protein_sequences_directory_path}\")\n",
        "    # Iterate through each file in the specified directory\n",
        "    for filename in os.listdir(protein_sequences_directory_path):\n",
        "        # Process files with common FASTA extensions\n",
        "        if filename.endswith(\".fasta\") or filename.endswith(\".fna\") or filename.endswith(\".fa\"):\n",
        "            # Extract the serogroup name from the filename (assuming filename is the serogroup name)\n",
        "            # Example: 'Ballum.fasta' -> 'Ballum'\n",
        "            serogroup_name = os.path.splitext(filename)[0]\n",
        "            file_path = os.path.join(protein_sequences_directory_path, filename)\n",
        "            print(f\"  - Reading file: {filename} for serogroup: {serogroup_name}\")\n",
        "\n",
        "            # Initialize a list to hold (header, sequence) tuples for the current serogroup\n",
        "            protein_sequences_by_serogroup[serogroup_name] = []\n",
        "\n",
        "            current_sequence = \"\"\n",
        "            current_header = \"\"\n",
        "            try:\n",
        "                # Read the sequences from the current FASTA file\n",
        "                with open(file_path, 'r') as f:\n",
        "                    for line in f:\n",
        "                        line = line.strip()\n",
        "                        if line.startswith('>'):\n",
        "                            # If there's a header and sequence from the previous entry, store it\n",
        "                            if current_header and current_sequence:\n",
        "                                # Store the previous sequence as a tuple (header, sequence)\n",
        "                                protein_sequences_by_serogroup[serogroup_name].append((current_header, current_sequence))\n",
        "\n",
        "                            # Start a new sequence entry\n",
        "                            current_header = line[1:] # Remove the '>' character\n",
        "                            current_sequence = \"\"\n",
        "                        elif line:\n",
        "                            # Append non-header lines to the current sequence\n",
        "                            current_sequence += line\n",
        "\n",
        "                    # Add the last sequence in the file after the loop finishes\n",
        "                    if current_header and current_sequence:\n",
        "                         protein_sequences_by_serogroup[serogroup_name].append((current_header, current_sequence))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  - Error reading file {filename}: {e}\")\n",
        "                continue # Continue to the next file even if one fails\n",
        "\n",
        "    print(f\"Loaded protein sequences for {len(protein_sequences_by_serogroup)} serogroups from directory.\")\n",
        "    # The dictionary 'protein_sequences_by_serogroup' now contains serogroup names as keys\n",
        "    # and a list of (header, sequence) tuples as values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q57_3pj9jRDF"
      },
      "outputs": [],
      "source": [
        "def parse_blast_output_and_create_feature_vector(blast_output_file, protein_sequences_by_serogroup, training_feature_columns):\n",
        "    print(f\"\\n🔍 Iniciando o processamento do arquivo BLAST: {os.path.basename(blast_output_file)}\")\n",
        "\n",
        "    try:\n",
        "        blast_cols = ['qseqid', 'sseqid', 'pident', 'length', 'qlen']\n",
        "        print(\"📥 Lendo o arquivo BLAST (formato outfmt 6: qseqid, sseqid, pident, length, qlen)...\")\n",
        "        blast_df = pd.read_csv(\n",
        "            blast_output_file,\n",
        "            sep='\\t',\n",
        "            header=None,\n",
        "            names=blast_cols,\n",
        "            dtype={'pident': float, 'length': int, 'qlen': int}\n",
        "        )\n",
        "\n",
        "        print(f\"📊 Dados carregados: {blast_df.shape[0]} linhas.\")\n",
        "        blast_df.fillna(0.0, inplace=True)\n",
        "\n",
        "        if blast_df.empty:\n",
        "            print(\"⚠️ Nenhum resultado encontrado no BLAST. Criando vetor de características zerado.\")\n",
        "            # Return a DataFrame with all training feature columns, filled with 0.0\n",
        "            return pd.DataFrame([{col: 0.0 for col in training_feature_columns}])\n",
        "\n",
        "        print(\"🧮 Calculando o pident corrigido para cada linha: pident * (length / qlen)...\")\n",
        "        blast_df['corrected_pident'] = blast_df.apply(\n",
        "            lambda row: row['pident'] * (row['length'] / row['qlen']) if row['qlen'] > 0 else 0.0,\n",
        "            axis=1\n",
        "        )\n",
        "        print(f\"✅ pident corrigido calculado. Exemplo:\\n{blast_df[['pident', 'length', 'qlen', 'corrected_pident']].head()}\")\n",
        "\n",
        "        print(\"\\n🔍 Selecionando o melhor hit (maior corrected_pident) por proteína consultada (qseqid)...\")\n",
        "        best_hits_per_protein = blast_df.loc[\n",
        "            blast_df.groupby('qseqid')['corrected_pident'].idxmax()\n",
        "        ].copy()\n",
        "        print(f\"✅ {len(best_hits_per_protein)} melhores hits selecionados.\")\n",
        "\n",
        "        print(\"🔡 Extraindo o nome do sorogrupo a partir do qseqid (última parte após '_')...\")\n",
        "        best_hits_per_protein['serogroup_name_from_qseqid'] = best_hits_per_protein['qseqid'].apply(\n",
        "            lambda x: x.split(\"_\")[-1] if \"_\" in x else 'Unknown'\n",
        "        )\n",
        "\n",
        "        print(\"📌 Verificando quais sorogrupos extraídos existem nas colunas de treinamento...\")\n",
        "        unique_qseqid_serogroups = best_hits_per_protein['serogroup_name_from_qseqid'].unique().tolist()\n",
        "        valid_serogroup_names_in_blast = [\n",
        "            name for name in unique_qseqid_serogroups if name in training_feature_columns\n",
        "        ]\n",
        "        print(f\"✔️ Sorogrupos válidos encontrados: {valid_serogroup_names_in_blast}\")\n",
        "\n",
        "        print(\"🧹 Filtrando apenas os melhores hits que pertencem aos sorogrupos válidos...\")\n",
        "        filtered_best_hits = best_hits_per_protein[\n",
        "            best_hits_per_protein['serogroup_name_from_qseqid'].isin(valid_serogroup_names_in_blast)\n",
        "        ].copy()\n",
        "        print(f\"✅ {filtered_best_hits.shape[0]} hits mantidos após o filtro.\")\n",
        "\n",
        "        print(\"📈 Agrupando por sorogrupo e calculando a MÉDIA do pident corrigido...\")\n",
        "        mean_corrected_pident_by_serogroup = (\n",
        "            filtered_best_hits.groupby('serogroup_name_from_qseqid')['corrected_pident'].mean()\n",
        "            if not filtered_best_hits.empty\n",
        "            else pd.Series(dtype=float)\n",
        "        )\n",
        "        print(\"📊 Média por sorogrupo calculada:\")\n",
        "        print(mean_corrected_pident_by_serogroup)\n",
        "\n",
        "        print(\"\\n📦 Iniciando vetor de características com zero para todos os sorogrupos esperados...\")\n",
        "        # Initialize with all training feature columns, set to 0.0\n",
        "        feature_vector_data = {col: 0.0 for col in training_feature_columns}\n",
        "\n",
        "        print(\"📝 Preenchendo o vetor de características com as médias calculadas por sorogrupo...\")\n",
        "        for serogroup, mean_value in mean_corrected_pident_by_serogroup.items():\n",
        "            # Only fill if the serogroup is in the expected training columns and not 'max'\n",
        "            if pd.notna(mean_value) and serogroup in feature_vector_data and serogroup != 'max':\n",
        "                print(f\"  - Atribuindo {mean_value:.2f} a '{serogroup}'\")\n",
        "                feature_vector_data[serogroup] = mean_value\n",
        "\n",
        "        # Remove the 'max' calculation and assignment\n",
        "        # print(\"\\n📈 Calculando a feature especial 'max' (maior valor entre as médias por sorogrupo)...\")\n",
        "        # if 'max' in training_feature_columns:\n",
        "        #     max_of_means = mean_corrected_pident_by_serogroup.max() if not mean_corrected_pident_by_serogroup.empty else 0.0\n",
        "        #     feature_vector_data['max'] = max_of_means if pd.notna(max_of_means) else 0.0\n",
        "        #     print(f\"  - Valor de 'max': {feature_vector_data['max']:.2f}\")\n",
        "\n",
        "\n",
        "        print(\"\\n📐 Construindo o DataFrame final com o vetor de características para predição...\")\n",
        "        feature_vector_df = pd.DataFrame([feature_vector_data])\n",
        "        # Ensure columns are in the correct order and fill missing with 0.0\n",
        "        feature_vector_df = feature_vector_df.reindex(columns=training_feature_columns, fill_value=0.0)\n",
        "        feature_vector_df = feature_vector_df.fillna(0.0)\n",
        "\n",
        "        print(f\"✅ Vetor de características criado com sucesso. Dimensão: {feature_vector_df.shape}\")\n",
        "        print(\"✅ Colunas preenchidas:\", feature_vector_df.columns.tolist())\n",
        "        print(\"✅ Valores finais:\")\n",
        "        print(feature_vector_df)\n",
        "\n",
        "        return feature_vector_df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ Erro: Arquivo não encontrado em {blast_output_file}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(\"❌ Erro inesperado durante o parsing:\")\n",
        "        traceback.print_exc()\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRkSBUsIjXvL"
      },
      "outputs": [],
      "source": [
        "def parse_fasta_to_tuples(fasta_path):\n",
        "    \"\"\"\n",
        "    Reads a .fasta file and returns a list of (header, sequence) tuples.\n",
        "\n",
        "    Args:\n",
        "        fasta_path (str): Path to the .fasta file.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples, where each tuple contains the sequence header\n",
        "              (without the '>' symbol) and the sequence string.\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    header = None\n",
        "    seq = \"\"\n",
        "\n",
        "    # Open and read the fasta file line by line\n",
        "    with open(fasta_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.startswith(\">\"):\n",
        "                # If a header and sequence were previously collected, store them\n",
        "                if header and seq:\n",
        "                    sequences.append((header, seq))\n",
        "                # Start a new sequence entry with the current header\n",
        "                header = line[1:]  # remove the \">\" character\n",
        "                seq = \"\"\n",
        "            else:\n",
        "                # Append non-header lines to the current sequence\n",
        "                seq += line\n",
        "        # Add the last sequence in the file after the loop finishes\n",
        "        if header and seq:\n",
        "            sequences.append((header, seq))\n",
        "\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUKsgW3kjchC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def build_sequence_dictionary(protein_sequences_directory_path):\n",
        "    \"\"\"\n",
        "    Reads all .fasta files in a directory and builds a dictionary mapping\n",
        "    simplified serogroup names (derived from filenames to match training features)\n",
        "    to lists of protein sequences.\n",
        "\n",
        "    Args:\n",
        "        protein_sequences_directory_path (str): Path to the directory containing\n",
        "                                                the protein sequence .fasta files.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are simplified serogroup names (matching\n",
        "              training features) and values are lists of (header, sequence) tuples\n",
        "              parsed from the .fasta files.\n",
        "    \"\"\"\n",
        "    protein_sequences_by_serogroup = {}\n",
        "\n",
        "    # Iterate through each file in the specified directory\n",
        "    for filename in os.listdir(protein_sequences_directory_path):\n",
        "        # Process files with common FASTA extensions\n",
        "        if filename.endswith(\".fasta\") or filename.endswith(\".fa\"):\n",
        "            fasta_path = os.path.join(protein_sequences_directory_path, filename)\n",
        "\n",
        "            # Extract the simplified serogroup name from the filename\n",
        "            # Assuming format like 'cl_1_Ballum_GCF_009884235.1.fasta'\n",
        "            # We want to extract 'Ballum'\n",
        "            parts = filename.split(\"_\")\n",
        "            if len(parts) > 2:\n",
        "                simplified_serogroup_name = parts[2] # Get the part after the second underscore\n",
        "                # Remove the extension and any trailing GCF/version info if present\n",
        "                simplified_serogroup_name = simplified_serogroup_name.split(\".\")[0]\n",
        "                # Handle cases like 'cl_2_Grippotyphosa.fasta'\n",
        "                if simplified_serogroup_name.endswith('fasta') or simplified_serogroup_name.endswith('fa'):\n",
        "                     simplified_serogroup_name = os.path.splitext(simplified_serogroup_name)[0]\n",
        "\n",
        "            else:\n",
        "                # Fallback or handle unexpected filename formats\n",
        "                print(f\"Warning: Unexpected filename format for extracting serogroup: {filename}. Using full name without extension as key.\")\n",
        "                simplified_serogroup_name = os.path.splitext(filename)[0]\n",
        "\n",
        "\n",
        "            # Parse the fasta file into a list of (header, sequence) tuples\n",
        "            sequence_list = parse_fasta_to_tuples(fasta_path)\n",
        "\n",
        "            # Add the parsed sequences to the dictionary under the simplified serogroup name key\n",
        "            protein_sequences_by_serogroup[simplified_serogroup_name] = sequence_list\n",
        "\n",
        "            print(f\"Processed {filename} and mapped to serogroup: {simplified_serogroup_name} ({len(sequence_list)} sequences)\")\n",
        "\n",
        "    return protein_sequences_by_serogroup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42af3ae3"
      },
      "source": [
        "## Data Loading and Preprocessing Functions\n",
        "\n",
        "This function preprocesses new data for prediction using a trained scaler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0Ak50oaiyli"
      },
      "outputs": [],
      "source": [
        "# Removed the scaler argument from the function definition\n",
        "def preprocess_for_prediction(table):\n",
        "    \"\"\"\n",
        "    Preprocess data for machine learning prediction.\n",
        "\n",
        "    Args:\n",
        "        table (pd.DataFrame): The input DataFrame for prediction.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (processed_table, original_indices) or (None, None) if there is an error.\n",
        "               processed_table (pd.DataFrame): Processed feature DataFrame.\n",
        "               original_indices (list): List of original row indices from the input DataFrame.\n",
        "    \"\"\"\n",
        "    print(\"Starting preprocessing for prediction...\")\n",
        "    # Drop the specified columns that are not features, ignoring errors if they don't exist\n",
        "    columns_to_exclude = ['Strain', 'Serogroup', 'Species','max']\n",
        "    processed_table = table.drop(columns=columns_to_exclude, errors='ignore').copy()\n",
        "    print(f\"  - Columns '{columns_to_exclude}' removed. Current shape: {processed_table.shape}\")\n",
        "\n",
        "    # Ensure all columns are numeric, coercing non-numeric values to NaN\n",
        "    print(\"  - Converting non-numeric columns to numeric...\")\n",
        "    non_numeric_columns = processed_table.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "    if non_numeric_columns:\n",
        "        print(f\"    - Non-numeric columns detected: {non_numeric_columns}\")\n",
        "        for col in non_numeric_columns:\n",
        "             processed_table[col] = pd.to_numeric(processed_table[col], errors='coerce')\n",
        "    print(\"  - Column conversion completed.\")\n",
        "\n",
        "    # Drop rows with any remaining missing values (NaNs)\n",
        "    rows_before_dropna = processed_table.shape[0]\n",
        "    processed_table = processed_table.dropna()\n",
        "    rows_after_dropna = processed_table.shape[0]\n",
        "    if rows_after_dropna < rows_before_dropna:\n",
        "        print(f\"  - Removed {rows_before_dropna - rows_after_dropna} rows with missing values.\")\n",
        "    print(f\"  - Shape after removing missing values: {processed_table.shape}\")\n",
        "\n",
        "    # Drop the 'class' column if it exists, as it's not a feature for prediction\n",
        "    if 'class' in processed_table.columns:\n",
        "        print(\"  - 'class' column found. Removing...\")\n",
        "        processed_table = processed_table.drop(columns=['class'])\n",
        "        print(f\"  - 'class' column removed. Current shape: {processed_table.shape}\")\n",
        "    else:\n",
        "        print(\"  - 'class' column not found. Proceeding.\")\n",
        "\n",
        "    # Debugging: Print columns before prediction to verify\n",
        "    print(\"\\n  - Columns in DataFrame before prediction:\")\n",
        "    print(processed_table.columns.tolist())\n",
        "    print(f\"  - Número de colunas antes de predição: {processed_table.shape[1]}\")\n",
        "\n",
        "    # Ensure the order of columns matches the training data features.\n",
        "    # This is crucial for consistent prediction.\n",
        "    # A robust approach requires saving and loading the list of training feature column names.\n",
        "    # Assuming for now that the remaining columns are in the correct relative order.\n",
        "\n",
        "    # Removed scaler application\n",
        "\n",
        "    print(\"Preprocessing for prediction completed.\")\n",
        "    # Return the feature matrix and the list of original indices for the rows that were kept\n",
        "    return processed_table, processed_table.index.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "332d1b54"
      },
      "source": [
        "## Loading Saved Model and Scaler and Protein Sequences\n",
        "\n",
        "This section loads the previously trained SVM model, the corresponding scaler, and the protein sequences for BLAST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cmrzg6cXitip",
        "outputId": "54c04f42-0397-4e57-d560-5ba10a7bace3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data from: serogroup_averages - Sheet2 (1).csv to identify features.\n",
            "\n",
            "Identified 22 training feature columns based on exclusion.\n",
            "Loading model from: ./Modelo_Leptospira/models/averages_with_serogroups_species - MLclass_model.pkl\n",
            "Model loaded successfully.\n",
            "Loading protein sequences from: ./Modelo_Leptospira/multi_fastas\n",
            "Processed cl_1_Ranarum_GCF_000332415.1.fasta and mapped to serogroup: Ranarum (117 sequences)\n",
            "Processed cl_1_Pyrogenes_GCF_022559725.1.fasta and mapped to serogroup: Pyrogenes (111 sequences)\n",
            "Processed cl_3_Hebdomadis_GCF_000244115.1.fasta and mapped to serogroup: Hebdomadis (83 sequences)\n",
            "Processed cl_1_Ballum_GCF_009884235.1.fasta and mapped to serogroup: Ballum (114 sequences)\n",
            "Processed cl_4_Tarassovi_GCF_024704545.1.fasta and mapped to serogroup: Tarassovi (90 sequences)\n",
            "Processed cl_2_Pomona_GCF_001857845.1.fasta and mapped to serogroup: Pomona (94 sequences)\n",
            "Processed cl_2_Autumnalis_GCF_022819425.1.fasta and mapped to serogroup: Autumnalis (102 sequences)\n",
            "Processed cl_4_Bataviae_GCF_014858865.1.fasta and mapped to serogroup: Bataviae (93 sequences)\n",
            "Processed cl_1_Icterohaemorrhagiae_GCF_000231175.1.fasta and mapped to serogroup: Icterohaemorrhagiae (107 sequences)\n",
            "Processed cl_1_Sarmin_GCF_030023765.1.fasta and mapped to serogroup: Sarmin (120 sequences)\n",
            "Processed cl_2_Djasiman_GCF_000216195.1.fasta and mapped to serogroup: Djasiman (83 sequences)\n",
            "Processed cl_1_Canicola_GCF_008831465.1.fasta and mapped to serogroup: Canicola (101 sequences)\n",
            "Processed cl_1_Manhao_GCF_000243815.2.fasta and mapped to serogroup: Manhao (121 sequences)\n",
            "Processed cl_2_Cynopteri_GCF_000243695.2.fasta and mapped to serogroup: Cynopteri (97 sequences)\n",
            "Processed cl_2_Panama_GCF_000306255.2.fasta and mapped to serogroup: Panama (97 sequences)\n",
            "Processed cl_3_Mini_GCF_000306675.2.fasta and mapped to serogroup: Mini (88 sequences)\n",
            "Processed cl_2_Australis_GCF_022819715.1.fasta and mapped to serogroup: Australis (106 sequences)\n",
            "Processed cl_3_Sejroe_GCF_003254845.1.fasta and mapped to serogroup: Sejroe (87 sequences)\n",
            "Processed cl_4_Shermani_GCF_000313175.2.fasta and mapped to serogroup: Shermani (62 sequences)\n",
            "Processed cl_2_Grippotyphosa_GCF_024526015.1.fasta and mapped to serogroup: Grippotyphosa (86 sequences)\n",
            "Processed cl_1_Celledoni_GCF_022344045.1.fasta and mapped to serogroup: Celledoni (123 sequences)\n",
            "Processed cl_1_Javanica_GCF_024722275.1.fasta and mapped to serogroup: Javanica (120 sequences)\n",
            "Loaded protein sequences for 22 serogroups.\n",
            "\n",
            "All necessary components (Model, Training Feature Columns, Protein Sequences) are loaded and ready for prediction.\n"
          ]
        }
      ],
      "source": [
        "# Check if the training data file exists\n",
        "if not os.path.exists(training_file_path):\n",
        "    print(f\"Error: Training data file not found at {training_file_path}\")\n",
        "    # Set variables to None or empty to indicate failure\n",
        "    training_feature_columns = None\n",
        "    loaded_model = None\n",
        "    # loaded_scaler = None # Remove the scaler variable\n",
        "    protein_sequences_by_serogroup = {}\n",
        "else:\n",
        "    # Load the data from the file path into a DataFrame to get training feature columns\n",
        "    print(f\"Loading training data from: {os.path.basename(training_file_path)} to identify features.\")\n",
        "    try:\n",
        "        df_from_file = pd.read_csv(training_file_path)\n",
        "\n",
        "        # Process the training data to identify the feature columns used during training.\n",
        "        # This assumes the 'verificar_e_transformar_dados' function processes the data\n",
        "        # in a way that the resulting DataFrame's columns (excluding the label) are the features.\n",
        "        # NOTE: The 'verificar_e_transformar_dados' function is not present in the provided cells.\n",
        "        # This part of the code might need adjustment based on how you identify training features.\n",
        "        # As a temporary placeholder, let's assume we can determine features after dropping\n",
        "        # Strain, Serogroup, Species, and class.\n",
        "\n",
        "        temp_df_processed = df_from_file.copy()\n",
        "        # Drop columns that are not features based on common non-feature columns and potentially 'class'\n",
        "        cols_to_drop_for_features = ['Strain', 'Serogroup', 'Species', 'class'] # Include 'class' if it's in the training data\n",
        "        # Filter columns to ensure they exist in the DataFrame before dropping\n",
        "        existing_cols_to_drop = [col for col in cols_to_drop_for_features if col in temp_df_processed.columns]\n",
        "        training_feature_columns = [col for col in temp_df_processed.columns if col not in existing_cols_to_drop]\n",
        "        # Remove 'max' from the training feature columns if it exists\n",
        "        if 'max' in training_feature_columns:\n",
        "             training_feature_columns.remove('max')\n",
        "\n",
        "\n",
        "        print(f\"\\nIdentified {len(training_feature_columns)} training feature columns based on exclusion.\")\n",
        "        # print(\"Training feature columns:\", training_feature_columns) # Uncomment for debugging\n",
        "\n",
        "        # Define the base name of the training file without extension\n",
        "        # training_base_name = os.path.splitext(os.path.basename(training_file_path))[0]\n",
        "        # NOTE: The model and scaler filenames might not directly correspond to the training data filename.\n",
        "        # It's safer to define their names explicitly or based on how they were saved.\n",
        "        # Assuming the saved model is 'modelo_sorogrupo.pkl' and scaler is 'scaler.pkl' as per the download list.\n",
        "        # Corrected filenames based on the actual files in the cloned repository\n",
        "        model_filename = \"averages_with_serogroups_species - MLclass_model.pkl\"\n",
        "\n",
        "\n",
        "\n",
        "        # Define the path to the directory where the model and scaler are saved.\n",
        "        # NOTE: This path is now set in the initial setup cell after downloading.\n",
        "        # model_directory_path = \"/content/drive/MyDrive/\" # Replace with your actual model directory path\n",
        "\n",
        "        # Define the full path to the saved model file\n",
        "        # Assuming the model file name is '<training_base_name>_model.pkl'\n",
        "        model_load_path = os.path.join(model_directory_path, model_filename)\n",
        "\n",
        "        # Load the saved model\n",
        "        # Check if the model file exists before attempting to load\n",
        "        if not os.path.exists(model_load_path):\n",
        "            print(f\"Error: Model file not found at {model_load_path}\")\n",
        "            loaded_model = None\n",
        "        else:\n",
        "            print(f\"Loading model from: {model_load_path}\")\n",
        "            loaded_model = joblib.load(model_load_path)\n",
        "            print(\"Model loaded successfully.\")\n",
        "\n",
        "         # Check if the protein sequences directory exists\n",
        "        if not os.path.isdir(protein_sequences_directory_path):\n",
        "             print(f\"Error: Protein sequences directory not found at {protein_sequences_directory_path}\")\n",
        "             protein_sequences_by_serogroup = {} # Initialize as empty if directory not found\n",
        "        else:\n",
        "            # Load protein sequences using the function\n",
        "            print(f\"Loading protein sequences from: {protein_sequences_directory_path}\")\n",
        "            # Use the build_sequence_dictionary function to load sequences from the local directory\n",
        "            protein_sequences_by_serogroup = build_sequence_dictionary(protein_sequences_directory_path)\n",
        "            if protein_sequences_by_serogroup:\n",
        "                print(f\"Loaded protein sequences for {len(protein_sequences_by_serogroup)} serogroups.\")\n",
        "            else:\n",
        "                print(\"No protein sequences were loaded.\")\n",
        "\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"Error: Training data file is empty at {training_file_path}\")\n",
        "        training_feature_columns = None\n",
        "        loaded_model = None\n",
        "        # loaded_scaler = None # Remove loaded_scaler\n",
        "        protein_sequences_by_serogroup = {}\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: A required file was not found.\")\n",
        "        training_feature_columns = None\n",
        "        loaded_model = None\n",
        "        # loaded_scaler = None # Remove loaded_scaler\n",
        "        protein_sequences_by_serogroup = {}\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during loading: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        training_feature_columns = None\n",
        "        loaded_model = None\n",
        "        # loaded_scaler = None # Remove loaded_scaler\n",
        "        protein_sequences_by_serogroup = {}\n",
        "\n",
        "\n",
        "# Add a check to ensure necessary components are loaded before proceeding\n",
        "# Removed loaded_scaler from the check\n",
        "if (loaded_model is None or training_feature_columns is None or not protein_sequences_by_serogroup):\n",
        "    print(\"\\nWarning: One or more necessary components (Model, Training Feature Columns, or Protein Sequences) could not be loaded. Please ensure the initial setup cell ran successfully and the required files were downloaded.\")\n",
        "else:\n",
        "     print(\"\\nAll necessary components (Model, Training Feature Columns, Protein Sequences) are loaded and ready for prediction.\")\n",
        "\n",
        "pass # Use pass if the cell is only for setup and doesn't produce direct output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e04c10a6"
      },
      "source": [
        "## Making Predictions and Evaluation\n",
        "\n",
        "This section uses the loaded model to make predictions on the preprocessed prediction data and evaluates the performance if true labels are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2-Xp9xLGi_b1",
        "outputId": "872038ac-89ec-4788-b9f3-a8d3bc417501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for .fas files in directory: ./Modelo_Leptospira/data\n",
            "Found 1 .fas or .fna files. Starting prediction for each genome.\n",
            "\n",
            "--- Attempting prediction for genome: 234_Noumea25.fas ---\n",
            "Starting prediction pipeline for genome: 234_Noumea25.fas\n",
            "Starting tblastn for genome archive: 234_Noumea25.fas\n",
            "  - Saving tblastn output to: /tmp/blast_output.txt\n",
            "  - Creating BLAST database...\n",
            "  - BLAST database created successfully.\n",
            "tblastn execution completed.\n",
            "\n",
            "🔍 Iniciando o processamento do arquivo BLAST: blast_output.txt\n",
            "📥 Lendo o arquivo BLAST (formato outfmt 6: qseqid, sseqid, pident, length, qlen)...\n",
            "📊 Dados carregados: 24547 linhas.\n",
            "🧮 Calculando o pident corrigido para cada linha: pident * (length / qlen)...\n",
            "✅ pident corrigido calculado. Exemplo:\n",
            "   pident  length  qlen  corrected_pident\n",
            "0  92.473      93   106         81.131972\n",
            "1  37.500      24   106          8.490566\n",
            "2  24.324      37   106          8.490453\n",
            "3  39.286      28   106         10.377434\n",
            "4  92.038     314   321         90.030941\n",
            "\n",
            "🔍 Selecionando o melhor hit (maior corrected_pident) por proteína consultada (qseqid)...\n",
            "✅ 1921 melhores hits selecionados.\n",
            "🔡 Extraindo o nome do sorogrupo a partir do qseqid (última parte após '_')...\n",
            "📌 Verificando quais sorogrupos extraídos existem nas colunas de treinamento...\n",
            "✔️ Sorogrupos válidos encontrados: ['Pyrogenes', 'Icterohaemorrhagiae', 'Canicola', 'Hebdomadis', 'Bataviae', 'Djasiman', 'Pomona', 'Grippotyphosa', 'Autumnalis', 'Cynopteri', 'Celledoni', 'Australis', 'Panama', 'Ballum', 'Javanica', 'Tarassovi', 'Sejroe', 'Mini', 'Ranarum', 'Shermani', 'Sarmin', 'Manhao']\n",
            "🧹 Filtrando apenas os melhores hits que pertencem aos sorogrupos válidos...\n",
            "✅ 1921 hits mantidos após o filtro.\n",
            "📈 Agrupando por sorogrupo e calculando a MÉDIA do pident corrigido...\n",
            "📊 Média por sorogrupo calculada:\n",
            "serogroup_name_from_qseqid\n",
            "Australis              43.935418\n",
            "Autumnalis             49.146864\n",
            "Ballum                 91.019155\n",
            "Bataviae               39.048023\n",
            "Canicola               63.428977\n",
            "Celledoni              63.640042\n",
            "Cynopteri              51.372830\n",
            "Djasiman               52.405411\n",
            "Grippotyphosa          55.323381\n",
            "Hebdomadis             39.307001\n",
            "Icterohaemorrhagiae    57.524278\n",
            "Javanica               79.840938\n",
            "Manhao                 61.419327\n",
            "Mini                   39.816014\n",
            "Panama                 46.396913\n",
            "Pomona                 52.407028\n",
            "Pyrogenes              67.133095\n",
            "Ranarum                60.605733\n",
            "Sarmin                 61.102291\n",
            "Sejroe                 43.086537\n",
            "Shermani               45.526272\n",
            "Tarassovi              53.713233\n",
            "Name: corrected_pident, dtype: float64\n",
            "\n",
            "📦 Iniciando vetor de características com zero para todos os sorogrupos esperados...\n",
            "📝 Preenchendo o vetor de características com as médias calculadas por sorogrupo...\n",
            "  - Atribuindo 43.94 a 'Australis'\n",
            "  - Atribuindo 49.15 a 'Autumnalis'\n",
            "  - Atribuindo 91.02 a 'Ballum'\n",
            "  - Atribuindo 39.05 a 'Bataviae'\n",
            "  - Atribuindo 63.43 a 'Canicola'\n",
            "  - Atribuindo 63.64 a 'Celledoni'\n",
            "  - Atribuindo 51.37 a 'Cynopteri'\n",
            "  - Atribuindo 52.41 a 'Djasiman'\n",
            "  - Atribuindo 55.32 a 'Grippotyphosa'\n",
            "  - Atribuindo 39.31 a 'Hebdomadis'\n",
            "  - Atribuindo 57.52 a 'Icterohaemorrhagiae'\n",
            "  - Atribuindo 79.84 a 'Javanica'\n",
            "  - Atribuindo 61.42 a 'Manhao'\n",
            "  - Atribuindo 39.82 a 'Mini'\n",
            "  - Atribuindo 46.40 a 'Panama'\n",
            "  - Atribuindo 52.41 a 'Pomona'\n",
            "  - Atribuindo 67.13 a 'Pyrogenes'\n",
            "  - Atribuindo 60.61 a 'Ranarum'\n",
            "  - Atribuindo 61.10 a 'Sarmin'\n",
            "  - Atribuindo 43.09 a 'Sejroe'\n",
            "  - Atribuindo 45.53 a 'Shermani'\n",
            "  - Atribuindo 53.71 a 'Tarassovi'\n",
            "\n",
            "📐 Construindo o DataFrame final com o vetor de características para predição...\n",
            "✅ Vetor de características criado com sucesso. Dimensão: (1, 22)\n",
            "✅ Colunas preenchidas: ['Australis', 'Autumnalis', 'Ballum', 'Bataviae', 'Canicola', 'Celledoni', 'Cynopteri', 'Djasiman', 'Grippotyphosa', 'Hebdomadis', 'Icterohaemorrhagiae', 'Javanica', 'Manhao', 'Mini', 'Panama', 'Pomona', 'Pyrogenes', 'Ranarum', 'Sarmin', 'Sejroe', 'Shermani', 'Tarassovi']\n",
            "✅ Valores finais:\n",
            "   Australis  Autumnalis     Ballum   Bataviae   Canicola  Celledoni  \\\n",
            "0  43.935418   49.146864  91.019155  39.048023  63.428977  63.640042   \n",
            "\n",
            "   Cynopteri   Djasiman  Grippotyphosa  Hebdomadis  ...     Manhao       Mini  \\\n",
            "0   51.37283  52.405411      55.323381   39.307001  ...  61.419327  39.816014   \n",
            "\n",
            "      Panama     Pomona  Pyrogenes    Ranarum     Sarmin     Sejroe  \\\n",
            "0  46.396913  52.407028  67.133095  60.605733  61.102291  43.086537   \n",
            "\n",
            "    Shermani  Tarassovi  \n",
            "0  45.526272  53.713233  \n",
            "\n",
            "[1 rows x 22 columns]\n",
            "\n",
            "--- Mean BLAST Pident per Serogroup for 234_Noumea25.fas ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Australis  Autumnalis     Ballum   Bataviae   Canicola  Celledoni  \\\n",
              "0  43.935418   49.146864  91.019155  39.048023  63.428977  63.640042   \n",
              "\n",
              "   Cynopteri   Djasiman  Grippotyphosa  Hebdomadis  ...     Manhao       Mini  \\\n",
              "0   51.37283  52.405411      55.323381   39.307001  ...  61.419327  39.816014   \n",
              "\n",
              "      Panama     Pomona  Pyrogenes    Ranarum     Sarmin     Sejroe  \\\n",
              "0  46.396913  52.407028  67.133095  60.605733  61.102291  43.086537   \n",
              "\n",
              "    Shermani  Tarassovi  \n",
              "0  45.526272  53.713233  \n",
              "\n",
              "[1 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-81026883-fb80-43de-9919-1f79d5936cff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Australis</th>\n",
              "      <th>Autumnalis</th>\n",
              "      <th>Ballum</th>\n",
              "      <th>Bataviae</th>\n",
              "      <th>Canicola</th>\n",
              "      <th>Celledoni</th>\n",
              "      <th>Cynopteri</th>\n",
              "      <th>Djasiman</th>\n",
              "      <th>Grippotyphosa</th>\n",
              "      <th>Hebdomadis</th>\n",
              "      <th>...</th>\n",
              "      <th>Manhao</th>\n",
              "      <th>Mini</th>\n",
              "      <th>Panama</th>\n",
              "      <th>Pomona</th>\n",
              "      <th>Pyrogenes</th>\n",
              "      <th>Ranarum</th>\n",
              "      <th>Sarmin</th>\n",
              "      <th>Sejroe</th>\n",
              "      <th>Shermani</th>\n",
              "      <th>Tarassovi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43.935418</td>\n",
              "      <td>49.146864</td>\n",
              "      <td>91.019155</td>\n",
              "      <td>39.048023</td>\n",
              "      <td>63.428977</td>\n",
              "      <td>63.640042</td>\n",
              "      <td>51.37283</td>\n",
              "      <td>52.405411</td>\n",
              "      <td>55.323381</td>\n",
              "      <td>39.307001</td>\n",
              "      <td>...</td>\n",
              "      <td>61.419327</td>\n",
              "      <td>39.816014</td>\n",
              "      <td>46.396913</td>\n",
              "      <td>52.407028</td>\n",
              "      <td>67.133095</td>\n",
              "      <td>60.605733</td>\n",
              "      <td>61.102291</td>\n",
              "      <td>43.086537</td>\n",
              "      <td>45.526272</td>\n",
              "      <td>53.713233</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 22 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81026883-fb80-43de-9919-1f79d5936cff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-81026883-fb80-43de-9919-1f79d5936cff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-81026883-fb80-43de-9919-1f79d5936cff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------\n",
            "  - Preparing feature vector for prediction...\n",
            "  - Feature vector ready for prediction.\n",
            "Prediction pipeline completed.\n",
            "\n",
            "--- All Prediction Results ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             Genome  Predicted_Class  Probability_Class_1  \\\n",
              "0  234_Noumea25.fas                1             0.979609   \n",
              "\n",
              "   Probability_Class_2  Probability_Class_3  Probability_Class_4  \n",
              "0             0.009178             0.003685             0.007529  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33fcb618-3be5-4641-99b5-4d4d7286cd26\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Genome</th>\n",
              "      <th>Predicted_Class</th>\n",
              "      <th>Probability_Class_1</th>\n",
              "      <th>Probability_Class_2</th>\n",
              "      <th>Probability_Class_3</th>\n",
              "      <th>Probability_Class_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>234_Noumea25.fas</td>\n",
              "      <td>1</td>\n",
              "      <td>0.979609</td>\n",
              "      <td>0.009178</td>\n",
              "      <td>0.003685</td>\n",
              "      <td>0.007529</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33fcb618-3be5-4641-99b5-4d4d7286cd26')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-33fcb618-3be5-4641-99b5-4d4d7286cd26 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-33fcb618-3be5-4641-99b5-4d4d7286cd26');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_20eb6d1f-2bcf-4c68-8b11-fc8f47e8feb3\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('final_prediction_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_20eb6d1f-2bcf-4c68-8b11-fc8f47e8feb3 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('final_prediction_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "final_prediction_df",
              "summary": "{\n  \"name\": \"final_prediction_df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Genome\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"234_Noumea25.fas\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Predicted_Class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Probability_Class_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.979608781438167,\n        \"max\": 0.979608781438167,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.979608781438167\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Probability_Class_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.009177995487996032,\n        \"max\": 0.009177995487996032,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.009177995487996032\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Probability_Class_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.003684641802440802,\n        \"max\": 0.003684641802440802,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.003684641802440802\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Probability_Class_4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0075285812713960345,\n        \"max\": 0.0075285812713960345,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0075285812713960345\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Check if the necessary components are loaded before proceeding with the prediction pipeline.\n",
        "# This includes the loaded model, the dictionary of protein sequences, and the list of training feature columns.\n",
        "# Removed loaded_scaler from the check\n",
        "if ('loaded_model' in locals() and loaded_model is not None and\n",
        "    'protein_sequences_by_serogroup' in locals() and protein_sequences_by_serogroup and\n",
        "    'training_feature_columns' in locals() and training_feature_columns is not None):\n",
        "\n",
        "    # Define the directory containing the new genome files\n",
        "    # This path is set in the initial setup cell after cloning.\n",
        "    genome_directory_path = os.path.join(cloned_repo_path, \"data\")\n",
        "\n",
        "    # Check if the genome directory exists\n",
        "    if not os.path.isdir(genome_directory_path):\n",
        "        print(f\"Error: Genome directory not found at {genome_directory_path}. Cannot proceed with prediction.\")\n",
        "    else:\n",
        "        print(f\"Checking for .fas files in directory: {genome_directory_path}\")\n",
        "        # List all files in the genome directory and filter for .fas and .fna files\n",
        "        genome_files = [os.path.join(genome_directory_path, f) for f in os.listdir(genome_directory_path) if f.endswith(('.fas','.fna'))]\n",
        "\n",
        "        if not genome_files:\n",
        "            print(f\"No .fas or .fna files found in {genome_directory_path}. No genomes to predict.\")\n",
        "        else:\n",
        "            print(f\"Found {len(genome_files)} .fas or .fna files. Starting prediction for each genome.\")\n",
        "            all_prediction_results = [] # List to store results from all genomes\n",
        "\n",
        "            # Loop through each found .fas or .fna file\n",
        "            for genome_file_path in genome_files:\n",
        "                print(f\"\\n--- Attempting prediction for genome: {os.path.basename(genome_file_path)} ---\")\n",
        "                # Run the prediction pipeline function for the current genome file.\n",
        "                # Removed loaded_scaler from the function call\n",
        "                prediction_results_df = predict_serogroup_from_genome(\n",
        "                    genome_file_path,\n",
        "                    protein_sequences_by_serogroup, # Dictionary of protein sequences by serogroup\n",
        "                    loaded_model, # Loaded machine learning model\n",
        "                    training_feature_columns  # List of training feature column names\n",
        "                )\n",
        "\n",
        "                # Append the results to the list if the pipeline was successful\n",
        "                if prediction_results_df is not None:\n",
        "                    all_prediction_results.append(prediction_results_df)\n",
        "                else:\n",
        "                    print(f\"\\nPrediction pipeline failed for {os.path.basename(genome_file_path)}.\")\n",
        "\n",
        "            # Concatenate all prediction results into a single DataFrame if any predictions were made\n",
        "            if all_prediction_results:\n",
        "                final_prediction_df = pd.concat(all_prediction_results, ignore_index=True)\n",
        "                print(\"\\n--- All Prediction Results ---\")\n",
        "                display(final_prediction_df)\n",
        "            else:\n",
        "                print(\"\\nNo successful predictions were made for any genome files.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    # Print an informative message if any of the necessary components are missing\n",
        "    # Removed Scaler from the message\n",
        "    print(\"\\nCannot run prediction pipeline: Necessary components (Model, Training Feature Columns, or Protein Sequences) are not loaded or defined. Please ensure the preceding setup and loading cells ran successfully and the required files exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13163d58"
      },
      "source": [
        "# Task\n",
        "Create a Gradio application that takes a DNA sequence as input and predicts the serogroup using the `predict_serogroup_from_genome` function, displaying the predicted serogroup and probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0906854"
      },
      "source": [
        "## Install gradio and biopython\n",
        "\n",
        "### Subtask:\n",
        "Install the necessary libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7ad64ae"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires installing the `gradio` and `biopython` libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fba02cd1",
        "outputId": "16d7172b-9b69-442f-e721-851efddefbba"
      },
      "source": [
        "%pip install gradio biopython"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.42.0)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.11/dist-packages (1.85)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "\n",
        "# --- Define the wrapper function for Gradio ---\n",
        "def predict_from_files(files):\n",
        "    results = []\n",
        "\n",
        "    if not files:\n",
        "        return \"⚠️ No files uploaded.\"\n",
        "\n",
        "    for f in files:\n",
        "        try:\n",
        "            prediction_df = predict_serogroup_from_genome(\n",
        "                f.name,\n",
        "                protein_sequences_by_serogroup,\n",
        "                loaded_model,\n",
        "                training_feature_columns\n",
        "            )\n",
        "            if prediction_df is not None:\n",
        "                results.append(prediction_df)\n",
        "        except Exception as e:\n",
        "            results.append(pd.DataFrame({\"Error\": [str(e)], \"File\": [f.name]}))\n",
        "\n",
        "    if results:\n",
        "        return pd.concat(results, ignore_index=True)\n",
        "    else:\n",
        "        return \"❌ No predictions could be made.\"\n",
        "\n",
        "# --- Gradio UI ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Leptospira Classifier Prototype\")\n",
        "    gr.Markdown(\"Upload `.fna` or `.fas` files and predict the **class/serogroup**.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(file_types=[\".fna\", \".fas\"], type=\"filepath\", file_count=\"multiple\", label=\"Upload genome files\")\n",
        "\n",
        "    with gr.Row():\n",
        "        predict_btn = gr.Button(\"Predict Class\")\n",
        "        clear_btn = gr.Button(\"Remove Files\")\n",
        "\n",
        "    output = gr.Dataframe(label=\"Prediction Results\")\n",
        "\n",
        "    # Button actions\n",
        "    predict_btn.click(predict_from_files, inputs=file_input, outputs=output)\n",
        "    clear_btn.click(fn=lambda: None, inputs=None, outputs=file_input)\n",
        "\n",
        "# Launch\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "KVE0tn0-VCmm",
        "outputId": "275664d6-0443-4731-f332-bb2d9ba09489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f3545e150043fbfae8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f3545e150043fbfae8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}